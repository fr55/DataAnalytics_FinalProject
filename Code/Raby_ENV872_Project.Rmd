---
output: 
  pdf_document:
    keep_tex: yes
    fig_caption: yes
    number_sections: yes
geometry: margin=2.54cm
title: Analysis of the effect of PM10 Concentration, Temperature, Geographic Location, Population, and distance to Electricity-Generation Combustion Points on the concentration of fine particulate matter (PM2.5) in North Carolina during the year 2018.
subtitle: https://github.com/fr55/DataAnalytics_FinalProject
author: Felipe Raby Amadori
abstract: "Experimental overview. This section should be no longer than 250 words."
fontsize: 12pt
mainfont: Times New Roman
editor_options: 
  chunk_output_type: console
---

<Information in these brackets are used for annotating the R Markdown file. They will not appear in the final version of the PDF document>

\newpage
\tableofcontents 
\newpage
\listoftables 
\newpage
\listoffigures 
\newpage

<Setup the global options for the R chunks in your document>

<Setup autoreferencing for figures and tables in your document>

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.path = './Output')
# Set your working directory

setwd("C:/Users/Felipe/OneDrive - Duke University/1. DUKE/1. Ramos 2 Semestre/EOS-872 Env. Data Analytics/DataAnalytics_FinalProject")

# Load your packages

library(FSA)
library(tidyverse)
library(RColorBrewer)
library(ggpubr)
library(viridis)
library(colormap)
library(leaflet)
library(sf)
library(tidyverse)
library(ggspatial)
library(mapview)
library(rvest)
library(dplyr)
library(FedData)
library(tiff)
library(gridExtra)
library(elevatr)
library(xtable)
library(car)
library(corrplot)
library(lubridate)
library (nlme)

# Set your ggplot theme

felipe_theme <- theme_light(base_size = 12) +
  theme(axis.text = element_text(color = "grey8"), 
        legend.position = "right", plot.title = element_text(hjust = 0.5)) 
theme_set(felipe_theme)

```


# Research Question and Rationale

<Paragraph detailing the rationale for your analysis. What is the significant application and/or interest in this topic? Connect to environmental topic(s)/challenge(s).>

Nowadays air pollution is one of the most relevant health issues in the world. It refers to the contamination of the air by chemicals, biological materials, and other types of pollutants that are harmful to human health. To solve the problem of air pollution, it's necessary to understand the problem, what are the causes, and search for solutions based on the findings.

Particulate matter with a diameter of less than 2.5 micrometers is called PM2.5, and it is a extremely harmful air pollutant because it consists of particles with diameters that are less than or equal to 2.5 microns in size, which can get deeply into the lung, and ultimately impair lung function.  

<Paragraph detailing your research question(s) and goals. What do you want to find out? Include a sentence (or a few) on the dataset you are using to answer this question - just enough to give your reader an idea of where you are going with the analysis.>

This study focus on trying to understand how PM2.5 concentration in North Carolina vary with temperature, PM10 concentration, zoning (piedmont, coastal, mountain), population, elevation, and distance to combustion points for electricity generation. This last variable was included because according to the EPA combustion for electricity generation is the major point-source sector for PM2.5 in the USA (EPA, 2019).

The research question is: What are the effects of temperature, PM10 concentration, zoning (piedmont, coastal, mountain), population, elevation, distance to combustion points for electricity generation, in PM2.5 concentrations within North Carolina in the year 2018? 

\newpage

# Dataset Information

<Information on how the dataset for this analysis were collected, the data contained in the dataset, and any important pieces of information that are relevant to your analyses. This section should contain much of same information as the README file for the dataset but formatted in a way that is more narrative.>

For the analysis the following datasets were considered:

## EPA PM2.5 Dataset

This dataset contains data from air quality monitoring of PM2.5 in North Carolina in 2018, and it was obtained using the Download Daily Data Tool in the United States Environmental Protection Agency (EPA) webpage https://www.epa.gov/outdoor-air-quality-data/download-daily-data where the options showed in Table \ref{tab:tab1} were selected:

```{r echo=FALSE, results="asis"}
#Creating Table
a <- data.frame("Option" = "Pollutant", "Selection" = "PM2.5")
b <- data.frame("Option" = "Year", "Selection" = "2018")
c <- data.frame("Option" = "Geographic Area", "Selection" = "North Carolina")
d <- data.frame("Option" = "Monitor Site", "Selection" = "All Sites")
e <- data.frame("Option" = "Download", "Selection" = "Download CSV (spreadsheet)")

summary_table <- rbind(a, b, c, d, e)

xtable1 <- xtable(summary_table, label="tab:tab1", caption="Selections")
print.xtable(xtable1, comment=FALSE, include.rownames=FALSE)
```

The downloaded file was saved in the project folder path ./Data/Raw/ as EPAair_PM25_NC2018_raw.csv on 2019-03-31.

### Data Content Information

The dataset contains daily mean PM2.5 concentration in ug/m3 in 2018. Data from 24 stations in 21 different counties of North Carolina with their location in NAD83 lat/long coordinates.

The dataset contains 19 columns, which are shown in Table \ref{tab:tab3}. Column names without description are self-explanatory.

```{r echo=FALSE, results="asis"}
#Creating Table
a <- data.frame("Column" = "Date", "Description" = "mm/dd/YY")
b <- data.frame("Column" = "Source", "Description" = "AQS (Air Quality System)")
c <- data.frame("Column" = "Site ID", "Description" = "A unique number identifying the site.")
d <- data.frame("Column" = "POC", "Description" = "“Parameter Occurrence Code”, distinguishes different instruments that measure the same parameter at the same site.")
e <- data.frame("Column" = "Daily Mean PM2.5 Concentration", "Description" = "")
f <- data.frame("Column" = "Units", "Description" = "Concentration Units")
t <- data.frame("Column" = "DAILY_AQI_VALUE", "Description" = "AQI = Air quality index")
h <- data.frame("Column" = "Site Name", "Description" = "")
i <- data.frame("Column" = "DAILY_OBS_COUNT", "Description" = "")
j <- data.frame("Column" = "PERCENT_COMPLETE", "Description" = "")
k <- data.frame("Column" = "AQS_PARAMETER_CODE", "Description" = "")
l <- data.frame("Column" = "AQS_PARAMETER_DESC", "Description" = "")
m <- data.frame("Column" = "CBSA_CODE", "Description" = "")
n <- data.frame("Column" = "CBSA_NAME", "Description" = "")
o <- data.frame("Column" = "STATE_CODE", "Description" = "")
g <- data.frame("Column" = "STATE", "Description" = "")
p <- data.frame("Column" = "COUNTY CODE", "Description" = "A unique number identifying the County.")
q <- data.frame("Column" = "COUNTY", "Description" = "")
r <- data.frame("Column" = "SITE_LATITUDE", "Description" = "NAD83")
s <- data.frame("Column" = "SITE_LONGITUDE", "Description" = "NAD83")

summary_table <- rbind(a, b, c, d, e, f, t, h, i, j, k, l, m, n, o, p, q, r, s)

xtable1 <- xtable(summary_table, label="tab:tab3", caption="Dataset content")
align(xtable1) <- "lp{2.5in}p{3.5in}"
print.xtable(xtable1, comment=FALSE, include.rownames=FALSE)
```

## EPA PM10 Dataset

This dataset contains data from air quality monitoring of PM10 in North Carolina in 2018, and it was obtained using the Download Daily Data Tool in the United States Environmental Protection Agency (EPA) webpage https://www.epa.gov/outdoor-air-quality-data/download-daily-data where the options showed in Table \ref{tab:tab4} were selected:

```{r echo=FALSE, results="asis"}
#Creating Table
a <- data.frame("Option" = "Pollutant", "Selection" = "PM10")
b <- data.frame("Option" = "Year", "Selection" = "2018")
c <- data.frame("Option" = "Geographic Area", "Selection" = "North Carolina")
d <- data.frame("Option" = "Monitor Site", "Selection" = "All Sites")
e <- data.frame("Option" = "Download", "Selection" = "Download CSV (spreadsheet)")

summary_table <- rbind(a, b, c, d, e)

xtable1 <- xtable(summary_table, label="tab:tab4", caption="Selections")
print.xtable(xtable1, comment=FALSE, include.rownames=FALSE)
```

The downloaded file was saved in the project folder path ./Data/Raw/ as EPAair_PM10_NC2018_raw.csv on 2019-03-31.

### Data Content Information

The dataset contains daily mean PM10 concentration in ug/m3 in 2018. Data from 9 stations in 8 different counties of North Carolina with their location in NAD83 lat/long coordinates.

The dataset contains 19 columns, which are shown in Table \ref{tab:tab5}. Column names without description are self-explanatory.

```{r echo=FALSE, results="asis"}
#Creating Table
a <- data.frame("Column" = "Date", "Description" = "mm/dd/YY")
b <- data.frame("Column" = "Source", "Description" = "AQS (Air Quality System)")
c <- data.frame("Column" = "Site ID", "Description" = "A unique number identifying the site.")
d <- data.frame("Column" = "POC", "Description" = "“Parameter Occurrence Code”, distinguishes different instruments that measure the same parameter at the same site.")
e <- data.frame("Column" = "Daily Mean PM10 Concentration", "Description" = "")
f <- data.frame("Column" = "Units", "Description" = "Concentration Units")
t <- data.frame("Column" = "DAILY_AQI_VALUE", "Description" = "AQI = Air quality index")
h <- data.frame("Column" = "Site Name", "Description" = "")
i <- data.frame("Column" = "DAILY_OBS_COUNT", "Description" = "")
j <- data.frame("Column" = "PERCENT_COMPLETE", "Description" = "")
k <- data.frame("Column" = "AQS_PARAMETER_CODE", "Description" = "")
l <- data.frame("Column" = "AQS_PARAMETER_DESC", "Description" = "")
m <- data.frame("Column" = "CBSA_CODE", "Description" = "")
n <- data.frame("Column" = "CBSA_NAME", "Description" = "")
o <- data.frame("Column" = "STATE_CODE", "Description" = "A unique number identifying the County.")
g <- data.frame("Column" = "STATE", "Description" = "")
p <- data.frame("Column" = "COUNTY CODE", "Description" = "A unique number identifying the County.")
q <- data.frame("Column" = "COUNTY", "Description" = "")
r <- data.frame("Column" = "SITE_LATITUDE", "Description" = "NAD83")
s <- data.frame("Column" = "SITE_LONGITUDE", "Description" = "NAD83")

summary_table <- rbind(a, b, c, d, e, f, t, h, i, j, k, l, m, n, o, p, q, r, s)

xtable1 <- xtable(summary_table, label="tab:tab5", caption="Dataset content")
align(xtable1) <- "lp{2.5in}p{3.5in}"
print.xtable(xtable1, comment=FALSE, include.rownames=FALSE)
```

## NOAA Average Temperature Dataset

This dataset contains data from temperature monitoring in North Carolina in 2018, and it was obtained using the Search Tool in the National Center for Environmental Information of the National Oceanic and Atmospheric Administration (NOAA) Webpage https://www.ncdc.noaa.gov/cdo-web. Options showed in Table \ref{tab:tab6} were selected:

```{r echo=FALSE, results="asis"}
#Creating Table
a <- data.frame("Option" = "Discover Data By", "Selection" = "Search Tool")
b <- data.frame("Option" = "Weather Observation Dataset", "Selection" = "Daily Summaries")
c <- data.frame("Option" = "Date Range", "Selection" = "2018-01-01 to 2018-12-31")
d <- data.frame("Option" = "Search For", "Selection" = "States")
e <- data.frame("Option" = "Search Term", "Selection" = "North Carolina")
f <- data.frame("Option" = "Output Format", "Selection" = "Custom GHCN-Daily CSV")
g <- data.frame("Option" = "Station Detail", "Selection" = "Station Name & Geographic Location")
h <- data.frame("Option" = "Data Type", "Selection" = "Average Temperature. (TAVG)")

summary_table <- rbind(a, b, c, d, e, f, g, h)

xtable1 <- xtable(summary_table, label="tab:tab6", caption="Selections")
print.xtable(xtable1, comment=FALSE, include.rownames=FALSE)
```

The downloaded file was saved in the project folder path ./Data/Raw/ as NOAA_TAVG_NC2018_raw.csv on 2019-03-28.

### Data Content Information

The dataset contains daily mean air temperature in Fahrenheit in 2018. Data from 39 stations in North Carolina with their location in NAD83 lat/long coordinates. No county information.

The dataset contains 7 columns, which are shown in Table \ref{tab:tab7}. Column names without description are self-explanatory.

```{r echo=FALSE, results="asis"}
#Creating Table
a <- data.frame("Column" = "STATION", "Description" = "A unique code identifying the site.")
b <- data.frame("Column" = "NAME", "Description" = "Station Name")
c <- data.frame("Column" = "Site ID", "Description" = "A unique number identifying the site.")
d <- data.frame("Column" = "LATITUDE", "Description" = "NAD83")
f <- data.frame("Column" = "LONGITUDE", "Description" = "NAD83")
g <- data.frame("Column" = "DATE", "Description" = "dd/mm/YY")
h <- data.frame("Column" = "TAVG", "Description" = "Daily Average Temperature in °F")

summary_table <- rbind(a, b, c, d, f, g, h)

xtable1 <- xtable(summary_table, label="tab:tab7", caption="Dataset content")
align(xtable1) <- "lp{2.5in}p{3.5in}"
print.xtable(xtable1, comment=FALSE, include.rownames=FALSE)
```

## US Census Bureau US counties shapefile

This dataset contains geographic and geometric information of all the counties of the US. The data is in NAD83 lat/long coordinates. The file was provided by John Fay in the Environmental Data Analytics (ENV 872L) course at Duke University, Spring 2019.

The files containing the information were saved in the project folder path ./Data/Spatial/ as cb_2017_us_county_20m on 2019-03-28.

### Data Content Information

The dataset contains geographic and geometric information of all the counties of the US in NAD83 lat/long coordinates.

The dataset contains 10 columns, which are shown in Table \ref{tab:tab8}. Column names without description are self-explanatory.

```{r echo=FALSE, results="asis"}
#Creating Table
a <- data.frame("Column" = "STATEFP", "Description" = "A unique number identifying the State.")
b <- data.frame("Column" = "COUNTYFP", "Description" = "County Federal Information Processing Standards (FIPS) Code")
c <- data.frame("Column" = "COUNTYNS", "Description" = "Provides the American National Standards Institute (ANSI) code for the county or equivalent entity, as used by GNIS.")
d <- data.frame("Column" = "AFFGEOID", "Description" = "AFF Summary Level Code")
e <- data.frame("Column" = "GEOID", "Description" = "NAD83")
f <- data.frame("Column" = "NAME", "Description" = "County Name")
g <- data.frame("Column" = "LSAD", "Description" = "Legal/statistical area description")
h <- data.frame("Column" = "ALAND", "Description" = "County Land Area in square meters")
i <- data.frame("Column" = "AWATER", "Description" = "County Water Area in square meters")
j <- data.frame("Column" = "Geometry", "Description" = "Geometry and geographic information")

summary_table <- rbind(a, b, c, d, e, f, g, h, i, j)

xtable1 <- xtable(summary_table, label="tab:tab8", caption="Dataset content")
align(xtable1) <- "lp{2.5in}p{3.5in}"
print.xtable(xtable1, comment=FALSE, include.rownames=FALSE)
```

## EPA combustion points for electricity generation in the US Dataset

This dataset contains facility-level locations for combustion points for electricity generation in the US, and it was obtained from the United States Environmental Protection Agency (EPA) webpage https://www3.epa.gov/air/emissions/where.htm. The Top PM2.5 emitting sectors link was selected. 

The downloaded file was saved in the project folder path ./Data/Raw/ as EPA_ElecGenComb_US_raw.kml on 2019-03-31.

### Data Content Information

The dataset is a kml file that contains combustion points for electricity generation in the US. The data is in WGS84 lat/long coordinates.

All data sets, variable, and files are named according to the following naming convention: _databasename_datatype_details_stage.format_, where: 

* databasename refers to the database from where the data originated

* datatype is a description of data 

* details are additional descriptive details, particularly important for processed data 

* stage refers to the stage in data management pipelines (e.g., raw, cleaned, temp or processed)

## Analized data structure

<Add a table that summarizes your data structure. This table can be made in markdown text or inserted as a `kable` function in an R chunk. If the latter, do not include the code used to generate your table.>

With these datasets an exploratory data analysis was done for the study. The datasets were wrangled in a dataframe called PM2.5_Full_Elev_utm and stored in the project processed folder in a file called PM2.5_Full_Elev_utm.shp. This dataframe has the data structure shown in Table \ref{tab:tab2}.

```{r echo=FALSE, results="asis"}
#Creating the Summary Table
summary_1 <- data.frame("Variable" = "Date", "Units" = "YY-mm-dd", "N°Elements" = "343", "Range" = "From 2018-01-01 to 2018-12-09", "Source.File"="EPAair_PM25_NC2018_raw.csv")

summary_2 <- data.frame("Variable" = "Site_ID", "Units" = "-", "N°Elements" = "24", "Range" = "-", "Source.File"="EPAair_PM25_NC2018_raw.csv")

summary_3 <- data.frame("Variable" = "COUNTY", "Units" = "-", "N°Elements" = "21", "Range" = "-", "Source.File"="EPAair_PM25_NC2018_raw.csv")

summary_4 <- data.frame("Variable" = "Population", "Units" = "People", "N°Elements" = "21", "Range" = "From 5,507 to 1,034,290", "Source.File"="https://en.wikipedia.org/")

summary_5 <- data.frame("Variable" = "Zone", "Units" = "-", "N°Elements" = "3", "Range" = "Coastal, Piedmont, and Mountains", "Source.File"="NC County Maps")

summary_6 <- data.frame("Variable" = "PM2_5", "Units" = "ug/m3", "N°Elements" = "6499", "Range" = "From -2.5 to 34.2", "Source.File"="EPAair_PM25_NC2018_raw.csv")

summary_7 <- data.frame("Variable" = "PM10", "Units" = "ug/m3", "N°Elements" = "926", "Range" = "From 0 to 35", "Source.File"="EPAair_PM10_NC2018_raw.csv")

summary_8 <- data.frame("Variable" = "TAVG", "Units" = "Farenheit", "N°Elements" = "4011", "Range" = "From 11 to 87", "Source.File"="NOAA_TAVG_NC2018_raw.csv")

summary_9 <- data.frame("Variable" = "Emiss_Dist", "Units" = "meters", "N°Elements" = "24", "Range" = "From 813.5 to 81800.9", "Source.File"="Self made")

summary_10 <- data.frame("Variable" = "Elevation", "Units" = "meters", "N°Elements" = "24", "Range" = "From 0.04 to 1418.8", "Source.File"="Package elevatr")

summary_table <- rbind(summary_1, summary_2, summary_3, summary_4, summary_5, summary_6, summary_7, summary_8, summary_9, summary_10)

xtable1 <- xtable(summary_table, label="tab:tab2", caption="Summary of data structure")
align(xtable1) <- "lp{0.6in}p{0.65in}p{0.55in}p{2in}p{1.8in}"

print.xtable(xtable1, comment=FALSE, include.rownames=FALSE, size="\\fontsize{9pt}{10pt}\\selectfont")
```

\newpage

# Exploratory Data Analysis and Wrangling

<Include R chunks for 5+ lines of summary code (display code and output), 3+ exploratory graphs (display graphs only), and any wrangling you do to your dataset(s).> 

<Include text sections to accompany these R chunks to explain the reasoning behind your workflow, and the rationale for your approach.>

##  EPA PM2.5 and PM10 Datasets

Uploading PM2.5 and PM10 2018 raw data files associated with EPA Air dataset and format date column. 

```{r}
EPA_AQPM25_NC2018_raw <- read.csv("./Data/Raw/EPAair_PM25_NC2018_raw.csv")
EPA_AQPM10_NC2018_raw <- read.csv("./Data/Raw/EPAair_PM10_NC2018_raw.csv")

#Formatting Dates
EPA_AQPM25_NC2018_raw$Date <- as.Date(EPA_AQPM25_NC2018_raw$Date, format = "%m/%d/%Y")
EPA_AQPM10_NC2018_raw$Date <- as.Date(EPA_AQPM10_NC2018_raw$Date, format = "%m/%d/%Y")
```

Data exploration of the PM2.5 and PM10 2018 raw data files associated with EPA Air dataset.

```{r}
dim(EPA_AQPM25_NC2018_raw)
dim(EPA_AQPM10_NC2018_raw)

str(EPA_AQPM25_NC2018_raw)
str(EPA_AQPM10_NC2018_raw)

colnames(EPA_AQPM25_NC2018_raw)
colnames(EPA_AQPM10_NC2018_raw)

summary(EPA_AQPM25_NC2018_raw)
summary(EPA_AQPM10_NC2018_raw)
```

Visual data exploration of the PM2.5 2018 raw data file in \autoref{PM2.5_freqpoly}, \autoref{PM2.5_boxplot}, and \autoref{PM2.5_scatterplot}.

```{r echo = FALSE, fig.cap = "PM2.5 NC 2018 frequency polygon. \\label{PM2.5_freqpoly}"}
ggplot() + 
  geom_freqpoly(data = EPA_AQPM25_NC2018_raw, aes(x = Daily.Mean.PM2.5.Concentration, color=COUNTY)) + 
  xlab(expression("PM 2.5 Concentration (\U003BCg/m3)")) +
  ggtitle("Exploratory Barplot")
```

```{r echo = FALSE, fig.cap = "PM2.5 NC 2018 boxplot. \\label{PM2.5_boxplot}"}
ggplot() + 
  geom_boxplot(data = EPA_AQPM25_NC2018_raw, aes(x = COUNTY,y = Daily.Mean.PM2.5.Concentration, color=COUNTY)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab(expression("PM 2.5 Concentration (\U003BCg/m3)")) +
  xlab(expression("County")) +
  ggtitle("Exploratory Boxplot")
```

```{r echo = FALSE, fig.cap = "PM2.5 NC 2018 scatterplot. \\label{PM2.5_scatterplot}"}
ggplot(EPA_AQPM25_NC2018_raw, aes(x=Date, y=Daily.Mean.PM2.5.Concentration)) +
  geom_point(aes(color = COUNTY), alpha=0.5) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab(expression("Date")) +
  ylab(expression("PM 2.5 Concentration (\U003BCg/m3)")) +
  geom_smooth(color ="black") +
  ggtitle("Exploratory Scatterplot")
```

Visual data exploration of the PM10 2018 raw data file in \autoref{PM10_scatterplot}.

```{r echo = FALSE, fig.cap = "PM10 NC 2018 scatterplot. \\label{PM10_scatterplot}"}
ggplot(EPA_AQPM10_NC2018_raw, aes(x=Date, y=Daily.Mean.PM10.Concentration)) +
  geom_point(aes(color = COUNTY), alpha=0.5) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab(expression("Date")) +
  ylab(expression("PM 10 Concentration (\U003BCg/m3)")) +
  geom_smooth(color ="black") +
  ggtitle("Exploratory Scatterplot")
```

Data wrangling of the PM2.5 and PM10 2018 raw data files.

```{r}
#Selecting Columns
EPA_AQ_PM25_NC2018_Temp <- select(EPA_AQPM25_NC2018_raw, Date, Site.ID,
                                Daily.Mean.PM2.5.Concentration, AQS_PARAMETER_DESC,
                                COUNTY:SITE_LONGITUDE)

#Changing column name
colnames(EPA_AQ_PM25_NC2018_Temp)[colnames(EPA_AQ_PM25_NC2018_Temp)
                                =="Daily.Mean.PM2.5.Concentration"]<-"Daily.Mean.Concentration"

#Selecting Columns
EPA_AQ_PM10_NC2018_Temp <- select(EPA_AQPM10_NC2018_raw, Date, Site.ID,
                                Daily.Mean.PM10.Concentration, AQS_PARAMETER_DESC,
                                COUNTY:SITE_LONGITUDE)

#Changing column name
colnames(EPA_AQ_PM10_NC2018_Temp)[colnames(EPA_AQ_PM10_NC2018_Temp)
                                =="Daily.Mean.PM10.Concentration"]<-"Daily.Mean.Concentration"

#Create AQS_PARAMETER_DESC Column with Contaminant description.
EPA_AQ_PM25_NC2018_Temp$AQS_PARAMETER_DESC <- "PM2.5"
EPA_AQ_PM10_NC2018_Temp$AQS_PARAMETER_DESC <- "PM10"

#Eliminates duplicate dates
EPA_AQ_PM25_NC2018_Cleaned <- EPA_AQ_PM25_NC2018_Temp [!duplicated(EPA_AQ_PM25_NC2018_Temp[c(1,2)]),]

EPA_AQ_PM10_NC2018_Cleaned <- EPA_AQ_PM10_NC2018_Temp [!duplicated(EPA_AQ_PM10_NC2018_Temp[c(1,2)]),]

# Combine the data.
EPA_AQ_PM2.5PM10_NC2018_Cleaned <- rbind(EPA_AQ_PM25_NC2018_Cleaned, EPA_AQ_PM10_NC2018_Cleaned)

#Save the data in the processed folder
write.csv(EPA_AQ_PM2.5PM10_NC2018_Cleaned,
         "./Data/Processed/EPA_AQ_PM2.5PM10_NC2018_Cleaned.csv")

#Spread PM2.5 and PM10
EPA_AQ_PM2.5PM10_NC2018_Spread <- 
  EPA_AQ_PM2.5PM10_NC2018_Cleaned %>%
  spread(AQS_PARAMETER_DESC, Daily.Mean.Concentration)

#Remove rows without PM2.5 data
EPA_AQ_PM2.5PM10_NC2018_Spread <- EPA_AQ_PM2.5PM10_NC2018_Spread[!is.na(EPA_AQ_PM2.5PM10_NC2018_Spread$PM2.5),]

#Convert the dataset to a spatially enabled "sf" data frame
PM2.5_PM10_sf <- st_as_sf(EPA_AQ_PM2.5PM10_NC2018_Spread,coords = c('SITE_LONGITUDE','SITE_LATITUDE'),crs=4269)

#Convert all to UTM Zone 17 (crs = 26917)
PM2.5_PM10_sf_utm <- st_transform(PM2.5_PM10_sf, c=26917)
```

In \autoref{PM2.5Stations} is presented the locations of the PM2.5 monitoring stations.

```{r echo = FALSE, fig.cap = "PM2.5 NC Monitoring Stations Previsualization. \\label{PM2.5Stations}"}
ggplot() + 
  #annotation_map_tile(zoom = 7) +
  geom_sf(data = PM2.5_PM10_sf_utm, color = 'Black', alpha=0.7) + 
  xlab(expression("Longitud")) +
  ylab(expression("Latitude")) +
  ggtitle("PM2.5 NC Monitoring Stations Previsualization")
```

##  North Carolina Counties Zoning, Geographic information, and Population Data

Downloading the list of North Carolina Counties and Population from a Wikipedia URL.

```{r}
#North Carolina Counties
url <- "https://en.wikipedia.org/wiki/List_of_counties_in_North_Carolina"
webpage <- read_html(url)

County_Name <- webpage %>% html_nodes("th:nth-child(1)") %>% html_text()
County_Population <- webpage %>% html_nodes("tr :nth-child(7)") %>% html_text() 

#Remove unwanted info and characters
County_Info <- data_frame(County = County_Name[9:108])
County_Info$County <- str_replace(County_Info$County, " County", "")
County_Info$County <- str_replace(County_Info$County, "\n", "")

Population <- data_frame(Population=County_Population[2:101])

County_Info <- cbind(County_Info, Population)

County_Info$Population <- str_replace(County_Info$Population,",","")
County_Info$Population <- str_replace(County_Info$Population,",","")

County_Info$Population <- as.numeric(County_Info$Population) 
```

Assigning the corresponding zone to each county. Info from: Rudersdorf, Amy. 2010. "NC County Maps." Government & Heritage Library, State Library of North Carolina.

```{r}
#North Carolina Zones
County_Info$Zone<-ifelse(County_Info$County == 'Ashe'|County_Info$County =='Alleghany'|County_Info$County =='Wilkes'|County_Info$County =='Watauga'|County_Info$County =='Avery'|County_Info$County =='Caldwell'|County_Info$County =='Mitchell'|County_Info$County =='Burke'|County_Info$County =='Yancey'|County_Info$County =='McDowell'|County_Info$County =='Rutherford'|County_Info$County =='Madison'|County_Info$County =='Buncombe'|County_Info$County =='Polk'|County_Info$County =='Henderson'|County_Info$County =='Haywood'|County_Info$County =='Transylvania'|County_Info$County =='Swain'|County_Info$County =='Jackson'|County_Info$County =='Graham'|County_Info$County =='Macon'|County_Info$County =='Cherokee'|County_Info$County =='Clay','Mountains',
                         ifelse(County_Info$County == 'Surry'|County_Info$County =='Stokes'|County_Info$County =='Rockingham'|County_Info$County =='Caswell'|County_Info$County =='Person'|County_Info$County =='Granville'|County_Info$County =='Vance'|County_Info$County =='Warren'|County_Info$County =='Yadkin'|County_Info$County =='Forsyth'|County_Info$County =='Guilford'|County_Info$County =='Alamance'|County_Info$County =='Orange'|County_Info$County =='Durham'|County_Info$County =='Franklin'|County_Info$County =='Alexander'|County_Info$County =='Iredell'|County_Info$County =='Davie'|County_Info$County =='Rowan'|County_Info$County =='Davidson'|County_Info$County =='Randolph'|County_Info$County =='Chatham'|County_Info$County =='Wake'|County_Info$County =='Catawba'|County_Info$County =='Cleveland'|County_Info$County =='Lincoln'|County_Info$County =='Gaston'|County_Info$County =='Mecklenburg'|County_Info$County =='Cabarrus'|County_Info$County =='Stanly'|County_Info$County =='Union'|County_Info$County =='Montgomery'|County_Info$County =='Anson'|County_Info$County =='Moore'|County_Info$County =='Lee'|County_Info$County =='Richmond','Piedmont',
                                ifelse(County_Info$County == 'Scotland'|County_Info$County =='Hoke'|County_Info$County =='Harnett'|County_Info$County =='Johnston'|County_Info$County =='Nash'|County_Info$County =='Halifax'|County_Info$County =='Northampton'|County_Info$County =='Robeson'|County_Info$County =='Cumberland'|County_Info$County =='Sampson'|County_Info$County =='Wayne'|County_Info$County =='Wilson'|County_Info$County =='Edgecombe'|County_Info$County =='Columbus'|County_Info$County =='Bladen'|County_Info$County =='Brunswick'|County_Info$County =='New Hanover'|County_Info$County =='Pender'|County_Info$County =='Duplin'|County_Info$County =='Onslow'|County_Info$County =='Lenoir'|County_Info$County =='Jones'|County_Info$County =='Carteret'|County_Info$County =='Greene'|County_Info$County =='Craven'|County_Info$County =='Pitt'|County_Info$County =='Pamlico'|County_Info$County =='Beaufort'|County_Info$County =='Martin'|County_Info$County =='Bertie'|County_Info$County =='Hyde'|County_Info$County =='Dare'|County_Info$County =='Tyrrell'|County_Info$County =='Washington'|County_Info$County =='Hertford'|County_Info$County =='Gates'|County_Info$County =='Currituck'|County_Info$County =='Camden'|County_Info$County =='Pasquotank'|County_Info$County =='Perquimans'|County_Info$County =='Chowan','Coastal','NoInfo')))
```

Data exploration of the County_Info dataframe.

```{r}
dim(County_Info)

str(County_Info)

colnames(County_Info)

summary(County_Info)

unique(County_Info$Zone)
```

Adding the County Information to the PM2.5_PM10_sf_utm dataframe.

```{r}
PM2.5_PM10_Info_sf_utm <- PM2.5_PM10_sf_utm %>%
left_join(County_Info, by = c("COUNTY"="County"))
```

##  US Census Bureau US counties shapefile

Reading the USA county shapefile, sub-setting for NC.

```{r read the USA county shapfile, subsetting for NC}
counties_sf<- st_read('./Data/Spatial/cb_2017_us_county_20m.shp') %>% 
  filter(STATEFP == 37) #Filter for just NC Counties

#CRS
st_crs(counties_sf) #crs=4269 = NAD83.
```

Converting the counties_sf to UTM Zone 17

```{r}
#Convert all to UTM Zone 17 (crs = 26917)
counties_sf_utm <- st_transform(counties_sf, c=26917)

#Adding the Zone Info
counties_sf_utm <- counties_sf_utm %>%
left_join(County_Info, by = c("NAME"="County"))
```

Data exploration of the County_Info dataframe.

```{r}
dim(counties_sf_utm)

str(counties_sf_utm)

colnames(counties_sf_utm)

summary(counties_sf_utm)
```

Visual data exploration of the counties_sf_utm dataframe in \autoref{Countyplot}.

```{r echo = FALSE, fig.cap = "Counties exploratory map. \\label{Countyplot}"}
ggplot() +
  geom_sf(data=counties_sf_utm, aes(fill=NAME)) +
  guides(fill=FALSE) +
  ggtitle("Exploratory Map")
```

```{r echo = FALSE, fig.cap = "NC Zoning exploratory map. \\label{Zoneplot}"}
ggplot() +
  geom_sf(data=counties_sf_utm, aes(fill=Zone)) +
  ggtitle("Zoning Exploratory Map")
```

##  NOAA Average Temperature Dataset

Reading the 2018 North Carolina Air Temperature data.

```{r}
#Read the 2018 Air Temperature data
NOAA_DTAVG_NC2018_raw <- read.csv("./Data/Raw/NOAA_TAVG_NC2018_raw.csv")
```

Data exploration of the NOAA_DTAVG_NC2018_raw dataframe.

```{r}
dim(NOAA_DTAVG_NC2018_raw)

str(NOAA_DTAVG_NC2018_raw)

colnames(NOAA_DTAVG_NC2018_raw)

summary(NOAA_DTAVG_NC2018_raw)
```

Data wrangling of the NOAA_DTAVG_NC2018_raw dataframe.

```{r}
#Remove stations without Temperature information
NOAA_DTAVG_NC2018_Complete <- na.omit(NOAA_DTAVG_NC2018_raw)

#Convert the dataset to a spatially enabled "sf" data frame
NOAA_DTAVG_NC2018_sf <- st_as_sf(NOAA_DTAVG_NC2018_Complete,coords = c('LONGITUDE','LATITUDE'),crs=4269) 

#Convert all to UTM Zone 17 (crs = 26917)
NOAA_DTAVG_NC2018_sf_utm <- st_transform(NOAA_DTAVG_NC2018_sf, c=26917)

#Formatting dates
NOAA_DTAVG_NC2018_sf_utm$DATE <- as.Date(NOAA_DTAVG_NC2018_sf_utm$DATE, format = "%d/%m/%Y")
```

The 2018 Air Temperature data does not have County information, so the location is used with the counties_sf_utm dataframe to locate the county of each station.

```{r}
#Adding the county and zone information to the Temperature dataframe

#Index of the matching feature
county_index <- st_nearest_feature(NOAA_DTAVG_NC2018_sf_utm, counties_sf_utm)

#Eliminates geo info
aux1 <- st_set_geometry(counties_sf_utm[county_index,"NAME"], value=NULL)

#adds the columns
NOAA_DTAVG_NC2018_sf_utm$COUNTY <- aux1$NAME

#Reordering
NOAA_DTAVG_NC2018_sf_utm <- NOAA_DTAVG_NC2018_sf_utm[,c(1,2,3,4,5,7,6)]

```

Visual data exploration of the 2018 North Carolina Air Temperature data in \autoref{Tempplot}, \autoref{Temp_freqpoly}, \autoref{Temp_boxplot}, and \autoref{Temp_scatterplot}..

```{r echo = FALSE, fig.cap = "Mean Annual Temperature exploratory map. \\label{Tempplot}"}
NOAA_DTAVG_NC2018_sf_utm_temp <- NOAA_DTAVG_NC2018_sf_utm %>%
  group_by(NAME) %>% 
  summarize(meanT = mean(TAVG))

ggplot() +
  geom_sf(data = counties_sf_utm, color = 'Black', fill = "antiquewhite") + 
  geom_sf(data = NOAA_DTAVG_NC2018_sf_utm_temp, aes(size = meanT, col = NAME, fill = NAME)) +
  guides(fill=FALSE, color=FALSE) +
  ggtitle("NC 2018 Temperature Exploratory Map, Mean Annual Temperature")
```

```{r echo = FALSE, fig.cap = "Daily Mean Temperature NC 2018 frequency polygon. \\label{Temp_freqpoly}"}
ggplot() + 
  geom_freqpoly(data = NOAA_DTAVG_NC2018_sf_utm, aes(x = TAVG, color=NAME)) + 
  xlab(expression("Daily Mean Temperature °F")) +
  guides(color=FALSE) +
  ggtitle("Exploratory Barplot")
```

```{r echo = FALSE, fig.cap = "Daily Mean Temperature NC 2018 boxplot. \\label{Temp_boxplot}"}
ggplot() + 
  geom_boxplot(data = NOAA_DTAVG_NC2018_sf_utm, aes(x = STATION,y = TAVG, color=NAME)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab(expression("Daily Mean Temperature °F")) +
  xlab(expression("Station")) +
  guides(color=FALSE) +
  ggtitle("Exploratory Boxplot")
```

```{r echo = FALSE, fig.cap = "Daily Mean Temperature NC 2018 scatterplot. \\label{Temp_scatterplot}"}
ggplot(NOAA_DTAVG_NC2018_sf_utm, aes(x=DATE, y=TAVG)) +
  geom_point(aes(color = STATION), alpha=0.5) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab(expression("Date")) +
  ylab(expression("Daily Mean Temperature °F")) +
  geom_smooth(color ="black") +
  guides(color=FALSE) +
  ggtitle("Exploratory Scatterplot")
```

Next, the temperature of the nearest Temperature Station is added to each PM2.5 Station in the PM2.5_PM10_Info_sf_utm dataframe.

```{r}
#Create a Data frame with only the PM2.5 station info
PM2.5_Stations <- PM2.5_PM10_Info_sf_utm %>%
  select(Site.ID, geometry) %>%
  subset(!duplicated(Site.ID))

#Distances bewteen the PM2.5 stations and the Temperature Stations
Nearest <- st_nearest_feature(PM2.5_Stations, NOAA_DTAVG_NC2018_sf_utm)

a <- length(unique(PM2.5_Stations$Site.ID))

NOAA_DTAVG_NC2018_sf_utm$NAME <- as.character(NOAA_DTAVG_NC2018_sf_utm$NAME)

#Assingning the nearest Temperature Station to each PM2.5 station.
for (i in 1:a){
  PM2.5_Stations$Temp_Est[i] <- NOAA_DTAVG_NC2018_sf_utm$NAME[Nearest[i]]
}

#Drop the geo data
aux2 <- st_set_geometry(PM2.5_Stations, value=NULL)

#Left_join the data
PM2.5_PM10_Temp_sf_utm <- PM2.5_PM10_Info_sf_utm %>%
left_join(aux2)

#Assingning the Temperature of the nearest Temperature Station to each PM2.5 station.

#Drops the geo data
aux3 <- st_set_geometry(NOAA_DTAVG_NC2018_sf_utm, value=NULL)

#Left_join the data
PM2.5_PM10_Temp_sf_utm <- PM2.5_PM10_Temp_sf_utm %>%
left_join(aux3, by = c("Temp_Est"="NAME", "Date"="DATE", "COUNTY")) %>%
  select(Date,Site.ID,COUNTY,Population,Zone,PM2.5,PM10,TAVG,geometry)
```

## EPA combustion points for electricity generation in the US Dataset

Reading the Electricity Generation via Combustion data.

```{r}
EPA_US_CombEmissions <- st_read("./Data/Raw/EPA_ElecGenComb_US_raw.kml")
```

Wrangling the data

```{r}
st_crs(EPA_US_CombEmissions) #crs=4326 = WGS 84

#Convert all to UTM Zone 17 (crs = 26917)
EPA_US_CombEmissions_utm <- st_transform(EPA_US_CombEmissions, c=26917)

#Clip the EPA_US_CombEmissions data set by the NC State boundary dataset

#First create a State_sf file
#Aggregate the data using group_by and summarize, just as you would a non-spatial dataframe
state_sf_utm <- st_union(counties_sf_utm)

#Eliminate the emission points outside NC
EPA_NC_CombEmissions_utm <- st_intersection(EPA_US_CombEmissions_utm,state_sf_utm) 
```

Visual data exploration of the EPA combustion points for electricity generation in the North Carolina in \autoref{Tempplot}, \autoref{Temp_freqpoly}, \autoref{Combplot}, and \autoref{Temp_scatterplot}..

```{r echo = FALSE, fig.cap = "Combustion points for electricity generation in the North Carolina. \\label{Combplot}"}

ggplot() +
  geom_sf(data = counties_sf_utm, color = 'Black', fill = "antiquewhite") + 
  geom_sf(data = EPA_NC_CombEmissions_utm) +
  ggtitle("NC Combustion Points for Electricity Generation Exploratory Map")
```

Now the distance between PM2.5 stations and Electricity Generation via Combustion points is determined and added to the PM2.5_PM10_Temp_sf_utm dataframe.

```{r}
#Distances between PM2.5 stations and Electricity Generation via Combustion points
Distances <- st_distance(PM2.5_Stations, EPA_NC_CombEmissions_utm)

a <- length(unique(PM2.5_Stations$Site.ID))

#Determining the minimum distance of each PM2.5 station to a combustion point in meters.
for (i in 1:a){
  PM2.5_Stations$Emiss_Dist[i] <- min(Distances[i,])
}

#Filling the PM2.5_PM10_Temp_sf_utm file with the distances

#Drops the geo data
aux4 <- PM2.5_Stations %>%
  st_set_geometry(value=NULL) %>% 
  select(Site.ID,Emiss_Dist)

#Left_join the data
PM2.5_Full_utm <- PM2.5_PM10_Temp_sf_utm %>%
left_join(aux4, by = c("Site.ID")) %>%
  select(Date,Site.ID,COUNTY,Population,Zone,PM2.5,PM10,TAVG,Emiss_Dist,geometry)
```

Finally, using the elevatr package, elevation information is added to the PM 2.5 stations in the PM2.5_Full_utm dataframe, creating the PM2.5_Full_Elev_utm, which is saved in the Project folder ./Data/Processed.

Elevations for the PM2.5 Stations
```{r, eval=FALSE}
prj_dd <- "+proj=utm +zone=17 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
PM2.5_Full_Elev_utm <- get_elev_point(PM2.5_Full_utm, prj = prj_dd, src = "epqs")

st_write(PM2.5_Full_Elev_utm,
         "./Data/Processed/PM2.5_Full_Elev_utm.shp", driver = "ESRI Shapefile")
```

```{r read the PM2.5_Full_Elev_utm shape file}
PM2.5_Full_Elev_utm <- st_read('./Data/Processed/PM2.5_Full_Elev_utm.shp') 
```

## Additional previsualization of the data

```{r echo = FALSE, fig.cap = "Daily PM2.5 Concentration. \\label{PM25plot}"}
PM2.5_Dist_Plot <- ggplot(PM2.5_Full_Elev_utm, aes(x=Date, y=PM2_5)) +
  geom_point(aes(color = Emiss_Dist)) +
  geom_smooth(color ="black") +
  xlab(expression("Date")) +
  ylab(expression("PM 2.5 Concentration (\U003BCg/m3)")) +
  scale_color_gradient(low="cyan", high="blue4", name = "Dist. to combustion point (m)")  +
  ggtitle("2018 Daily PM2.5 concentration, North Carolina") +
  geom_hline(yintercept=12, linetype="dashed", color = "black", size = 1)

PM2.5_Zone_Plot <- ggplot(PM2.5_Full_Elev_utm, aes(x=Date, y=PM2_5)) +
  geom_point(aes(color = Zone)) +
  geom_smooth(color ="black") +
  xlab(expression("Date")) +
  ylab(expression("PM 2.5 Concentration (\U003BCg/m3)")) +
  scale_fill_manual("red","blue","yellow")  +
  ggtitle("2018 Daily PM2.5 concentration, North Carolina") +
  geom_hline(yintercept=12, linetype="dashed", color = "black", size = 1)

PM2.5_Elevation_Plot <- ggplot(PM2.5_Full_Elev_utm, aes(x=Date, y=PM2_5)) +
  geom_point(aes(color = elevation)) +
  geom_smooth(color ="black") +
  xlab(expression("Date")) +
  ylab(expression("PM 2.5 Concentration (\U003BCg/m3)")) +
  scale_color_gradient(low="cyan", high="blue4", name = "Elev. (m)")  +
  ggtitle("2018 Daily PM2.5 concentration, North Carolina")

PM2.5_Temp_Plot <- ggplot(PM2.5_Full_Elev_utm, aes(x=Date, y=PM2_5)) +
  geom_point(aes(color = TAVG)) +
  geom_smooth(color ="black") +
  xlab(expression("Date")) +
  ylab(expression("PM 2.5 Concentration (\U003BCg/m3)")) +
  scale_color_gradient(low="cyan", high="blue4", name = "Temp (°F)")  +
  ggtitle("2018 Daily PM2.5 concentration, North Carolina")

PM2.5_Pop_Plot <- ggplot(PM2.5_Full_Elev_utm, aes(x=Date, y=PM2_5)) +
  geom_point(aes(color = Population)) +
  geom_smooth(color ="black") +
  xlab(expression("Date")) +
  ylab(expression("PM 2.5 Concentration (\U003BCg/m3)")) +
  scale_color_gradient(low="cyan", high="blue4", name = "Population")  +
  ggtitle("2018 Daily PM2.5 concentration, North Carolina")

grid.arrange(PM2.5_Dist_Plot, PM2.5_Zone_Plot, PM2.5_Elevation_Plot, PM2.5_Temp_Plot, PM2.5_Pop_Plot, nrow = 5)
```

\newpage

# Analysis
<Include R chunks for 3+ statistical tests (display code and output) and 3+ final visualization graphs (display graphs only).>

<Include text sections to accompany these R chunks to explain the reasoning behind your workflow, rationale for your approach, and the justification of meeting or failing to meet assumptions of tests.>

In 2012, the United States Environmental Protection Agency (USEPA) established a complementary primary regulatory standard for PM2.5 at a concentration of 12 micrograms per cubic meter, ug/m3< therefore, the first statistical test should look at the standard in each station.

## One-sample t-test

The first statistical analysis will test the null hypothesis that the mean of the PM 2.5 concentrations in North Carolina are below the regulatory standard of 12 micrograms per cubic meter for the three geographical zones (Coastal, Piedmont, and Mountains).

First the assumption of normal distribution is evaluated.

```{r}
PM2.5_Coastal <- PM2.5_Full_Elev_utm$PM2.5[PM2.5_Full_Elev_utm$Zone == "Coastal"]
PM2.5_Coastal <- as.data.frame(PM2.5_Coastal)

PM2.5_Piedmont <- PM2.5_Full_Elev_utm$PM2.5[PM2.5_Full_Elev_utm$Zone == "Piedmont"]
PM2.5_Piedmont <- as.data.frame(PM2.5_Piedmont)

PM2.5_Mountains <- PM2.5_Full_Elev_utm$PM2.5[PM2.5_Full_Elev_utm$Zone == "Mountains"]
PM2.5_Mountains <- as.data.frame(PM2.5_Mountains)

shapiro.test(PM2.5_Coastal$PM2.5_Coastal)
shapiro.test(PM2.5_Piedmont$PM2.5_Piedmont)
shapiro.test(PM2.5_Mountains$PM2.5_Mountains)
```

The Shapiro-Wilk normality test says that the PM 2.5 concentrations data in the three NC zones are significantly different from a normal distribution ( Coastal: Shapiro-Wilk normality test, W = 0.9786, p-value < 0.0001; Piedmont: Shapiro-Wilk normality test, W = 0.96427, p-value < 0.0001; Mountain: Shapiro-Wilk normality test, W = 0.95824, p-value < 0.0001).

Next, a graphical analysis of the data is performed.

```{r echo = FALSE, fig.cap = "Daily PM2.5 Concentration Coastal Histogram. \\label{Coasthistplot}"}
ggplot(PM2.5_Coastal, aes(x = PM2.5_Coastal)) +
  geom_histogram(stat = "count")
```

```{r echo = FALSE, fig.cap = "Daily PM2.5 Concentration Coastal qqplot. \\label{Coastqqplot"}
qqnorm(PM2.5_Coastal$PM2.5_Coastal); qqline(PM2.5_Coastal$PM2.5_Coastal)
```

```{r echo = FALSE, fig.cap = "Daily PM2.5 Concentration Coastal Histogram. \\label{Piedmonthistplot}"}
ggplot(PM2.5_Piedmont, aes(x = PM2.5_Piedmont)) +
  geom_histogram(stat = "count")
```

```{r echo = FALSE, fig.cap = "Daily PM2.5 Concentration Coastal qqplot. \\label{Piedmontqqplot"}
qqnorm(PM2.5_Piedmont$PM2.5_Piedmont); qqline(PM2.5_Piedmont$PM2.5_Piedmont)
```

```{r echo = FALSE, fig.cap = "Daily PM2.5 Concentration Coastal Histogram. \\label{Mounthistplot}"}
ggplot(PM2.5_Mountains, aes(x = PM2.5_Mountains)) +
  geom_histogram(stat = "count")
```

```{r echo = FALSE, fig.cap = "Daily PM2.5 Concentration Coastal qqplot. \\label{Mountqqplot"}
qqnorm(PM2.5_Mountains$PM2.5_Mountains); qqline(PM2.5_Mountains$PM2.5_Mountains)
```

In \autoref{Coasthistplot},\autoref{Coastqqplot}, \autoref{Piedmontqqplot}, \autoref{Piedmontqqplot}, \autoref{Mountqqplot}, and \autoref{Mountqqplot} it can be seen that the data have more than one peak, heavier tails, and longer tail to the right than a normal distribution; nevertheless, environmental data often violate the assumptions of normality and the histograms fairly resembles a bell curve, so a t-test is performed anyway.

```{r}
t.test(PM2.5_Coastal$PM2.5_Coastal, mu = 12, alternative = "less")
```

According to the One Sample t-test, NC Coastal PM 2.5 concentrations in 2018 were significantly lower than 12 ug/m3 (one sample t-test; t = --69.865, df =1754, p<0.0001).

```{r}
t.test(PM2.5_Piedmont$PM2.5_Piedmont, mu = 12, alternative = "less")
```

According to the One Sample t-test, NC Piedmont PM 2.5 concentrations in 2018 were significantly lower than 12 ug/m3 (one sample t-test; t = -67.406, df = 4433, p-value < 0.0001).

```{r}
t.test(PM2.5_Mountains$PM2.5_Mountains, mu = 12, alternative = "less")
```

According to the One Sample t-test, NC Mountains PM 2.5 concentrations in 2018 were significantly lower than 12 ug/m3 (one sample t-test; t = -53.529, df = 1270, p<0.0001).

According to the data and the One sample t-tests performed, the PM 2.5 concentrations in North Carolina are below the regulatory standard of 12 micrograms per cubic meter for the three geographical zones (Coastal, Piedmont, and Mountains).

In \autoref{PM2.5Standardplot} is presented a visualization of the PM 2.5 concentrations in North Carolina data in comparison with the North Carolina regulatory standard.

```{r echo = FALSE, fig.cap = "Daily PM2.5 Concentration, North Carolina. \\label{PM2.5Standardplot}"}

ggplot(PM2.5_Full_Elev_utm, aes(x=Date, y=PM2.5)) +
  geom_point(aes(color = PM2.5), alpha = 0.4) +
  geom_hline(yintercept=12, linetype="dashed", color = "black", size = 1) +
  annotate("text", x = as.Date(c("2018-04-15")), y = 13.5, label = "NC regulatory standard = 12 ug/m3") +
  xlab(expression("Date")) +
  ylab(expression("PM 2.5 Concentration (\U003BCg/m3)")) +
  scale_color_distiller(palette = "Set1") +
  ggtitle("2018 Daily PM2.5 concentration, North Carolina")


```

## One-way ANOVA

Now that is known that the PM 2.5 concentrations in North Carolina in 2018 were below the regulatory standard of 12 micrograms per cubic meter for the three geographical zones, it is of interest to check if there are significant differences between the means of PM 2.5 concentrations for the same three geographical zones. 

Therefore, the second statistical analysis will test the null hypothesis that the mean of the PM 2.5 concentrations in North Carolina are equal for the three geographical zones (Coastal, Piedmont, and Mountains).

For this, a One-way ANOVA test is performed. This test requires a second assumption to be complied, which is that the variance of the groups is equal across groups. Taking into account that the data are not perfectly normal, to test for the homogeneity of variance across groups a Fligner-Killeen test is used. This test is a non-parametric test, which is very robust against departures from normality.

```{r}
fligner.test(PM2.5_Full_Elev_utm$PM2.5 ~ as.factor(PM2.5_Full_Elev_utm$Zone))
```

The Fligner-Killeen test of homogeneity of variances says that the variance across groups is not homogeneous, but with a p-value close to 0.05 (med chi-squared = 7.6425, df = 2, p-value = 0.0219 < 0.05). 

For this reason, for testing if there are significant differences between the means of PM 2.5 concentrations for the same three geographical zones, it is used a One-way ANOVA test and a Non-parametric equivalent of ANOVA, the Kruskal-Wallis Test.

```{r}
PM2.5.anova <- lm(PM2.5_Full_Elev_utm$PM2.5 ~ as.factor(PM2.5_Full_Elev_utm$Zone))
anova(PM2.5.anova)

ktest_PM2.5 <- kruskal.test(PM2.5_Full_Elev_utm$PM2.5 ~ as.factor(PM2.5_Full_Elev_utm$Zone))
ktest_PM2.5
```

According to both test, there is a significant difference between the means of PM 2.5 concentrations for the same three geographical zones (ANOVA; F = 238.95, df = 7457, p<2.2e-16) and (Kruskal-Wallis chi-squared = 471.69, df = 2, p-value < 2.2e-16)

To analyze which zones are different, two *post hoc* tests were used, a Tukey multiple comparisons of means test for ANOVA and a Dunn's test for Kruskal-Wallis.

```{r}
# Run a post-hoc test for pairwise differences
TukeyHSD(aov(PM2.5_Full_Elev_utm$PM2.5 ~ as.factor(PM2.5_Full_Elev_utm$Zone))) 

dunnTest(PM2.5_Full_Elev_utm$PM2.5 ~ as.factor(PM2.5_Full_Elev_utm$Zone))
```

Both test give as a result that there is a significant difference between the means of PM 2.5 concentrations between the Piedmont and both the Mountains and the Coastal zone (TukeHSD; p-value<0.0001) and (Dunn; p-value<0.0001). There is no significant difference between the means of PM 2.5 concentrations between the Mountains and the Coastal zone (TukeHSD; p-value>0.05) and (Dunn; p-value>0.05).

To explore graphically these results and present a visualization of the results of this test, in \autoref{BOX_plot}} is presented a Boxplot for the Daily PM2.5 Concentration for the three North Carolina Zones.

```{r echo = FALSE, fig.cap = "Daily PM2.5 Concentration Zones Boxplot. \\label{BOX_plot}"}
ggplot() + 
  geom_boxplot(data = PM2.5_Full_Elev_utm, aes(x = Zone,y = PM2.5, fill=Zone)) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = c("#e66101", "#5e3c99", "deepskyblue2")) +
  guides(fill=FALSE) +
  ylab(expression("PM 2.5 Concentration (\U003BCg/m3)")) +
  xlab(expression("North Carolina Zone")) +
  ggtitle("PM 2.5 Concentration by zone")
```

## Mixed-effect model

Finally, to answer the specific research question of this study (What are the effects of temperature, PM10 concentration, zoning (piedmont, coastal, mountain), population, elevation, distance to combustion points for electricity generation, in PM2.5 concentrations within North Carolina in the year 2018?), an analysis of covariance (ANCOVA) test was performed.   

In the model, a variable Week is created and defined as a random variable because it is not of interest the specific effects of each week on PM 2.5 concentrations but the variability among weeks. The hierarchical model is set up to deal with any variability associated with seasonality.

In this study multiple multiple explanatory variables are being considered at a time in the model, so to check the model is not over-parameterized the Akaike's Information Criterion (AIC) was used.

```{r}
PM252018.model <- lm(data = PM2.5_Full_Elev_Week_utm, PM2.5 ~ Population + as.factor(Zone) + PM10 + TAVG + Emiss_Dist + elevation)

step(PM252018.model) # the lower AIC value the better

```

The variable distance to combustion emission point was dropped by the Akaike's Information Criterion (AIC). According to this result, the set of explanatory variables that are best suited to predict PM 2.5 concentration are Population, Zone, PM10, TAVG, and elevation. Next, a multiple regression is performed on the recommended set of variables.

```{r}
PM252018.model <- lm(data = PM2.5_Full_Elev_Week_utm, PM2.5 ~ Population + as.factor(Zone) + PM10 + TAVG + elevation)
anova(PM252018.model)
```

In the full model, only Zone is not statistically significant. The model is run without that variable.

```{r}
PM252018.model <- lm(data = PM2.5_Full_Elev_Week_utm, PM2.5 ~ Population + PM10 + TAVG + elevation)
summary(PM252018.model)
```

Plotting the diagnostic plots of the model in \autoref{DiagPlot}} to check the assumptions.

```{r echo = FALSE, fig.cap = "Model diagnostic plots. \\label{DiagPlot}"}
par(mfrow = c(2,2))
plot(PM252018.model)
```

In \autoref{DiagPlot}}, in the residuals vs fitted plot it can be seen an slightly increasing variance pattern but overall we can say that the points appear to be randomly scattered around the centerline. Also, in the normal Q-Q plot the points appear to follow an straight line (except for the extremes which is common for environmental data), so the graph is accepted. In the Scale- Location plot the points appear to be randomly scattered, so there is no evidence of heteroscedasticity and the homogeneous variance assumption is met. In the Residuals versus Leverage plots we check for outliers to verify that no single data point is so influential that leaving it out changes the structure of the model. 

To make the coefficients easier to manipulate, let's first save the regression coefficients from our model model4 to a variable.

From the model we got that the intercept, 4.168, is the PM 2.5 mean daily concentration (ug/m3) when all the other variables are zero. We reject the null hypothesis of no effect of the explanatory variables on the response variable. 

Population significantly increases the PM 2.5 mean daily concentration (ug/m3). With an increase of population by 1 the PM 2.5 concentration is increased by 1.450e-06 ug/m3 (t =  7.111, df = 1682, p < 0.001). 

PM10 daily mean concentration (ug/m3) significantly increases the PM 2.5 mean daily concentration (ug/m3). With an increase of PM10 daily mean concentration by 1 ug/m3 the PM 2.5 concentration is increased by 4.709e-01 ug/m3 (t =  46.246, df = 1682, p < 0.001). 

Daily Average Temperature (°F) significantly decreases the PM 2.5 mean daily concentration (ug/m3). With an increase of daily average temperature by 1 °F the PM 2.5 concentration is decreased by 3.096e-02 ug/m3 (t =  -8.546, df = 1682, p < 0.001). 

Finally, elevation (meters) significantly decreases the PM 2.5 mean daily concentration (ug/m3). With an increase of elevation by 1 meter the PM 2.5 concentration is decreased by -1.281e-02 ug/m3 (t =  -14.691, df = 1682, p < 0.001). The Adjusted R-squared = 0.5696, which is the fraction of total variance explained by the model.

The model explain 56.96% of the observed variance. The final linear equation to predict PM 2.5 mean daily concentration (ug/m3) from the explanatory variables is:

$$ PM2.5 = \ 4.168 + \ 0.00000145*Population + \ 0.471*PM10  \ -0.03096*T° \ -0.0128*Elev + \epsilon $$

```{r echo = FALSE, fig.cap = "Daily PM2.5 Concentration vs Model Variables. \\label{Final_plot}"}
ggplot(PM2.5_Full_Elev_utm, aes(x=Date, y=PM2.5)) +
  geom_point(aes(color = PM2.5), alpha = 0.4) +
  geom_hline(yintercept=12, linetype="dashed", color = "black", size = 1) +
  annotate("text", x = as.Date(c("2018-04-15")), y = 13.5, label = "NC regulatory standard = 12 ug/m3") +
  xlab(expression("Date")) +
  ylab(expression("PM 2.5 Concentration (\U003BCg/m3)")) +
  scale_color_distiller(palette = "Set1") +
  ggtitle("2018 Daily PM2.5 concentration, North Carolina")
```




FRA: This model structure should look familiar, with a typical linear model structure and dataframe defined. The addition here is that we have defined Week as a random variable. Essentially, we are interested not in the specific effects of each week but in the variability among weeks, so we have defined it as a random effect (essentially coming from a larger distribution of seasonal variability).

\newpage

# Summary and Conclusions
<Summarize your major findings from your analyses. What conclusions do you draw from your findings? Make sure to apply this to a broader application for the research question you have answered.>

For conclusion. Depending in the results of the linear model we can say also that in can be piedmont because more points or more population. 

```{r}
aux2 <- st_set_geometry(PM2.5_Stations, value=NULL)

PopulationbyZone <-
  PM2.5_Full_Elev_utm %>%
  group_by(Zone) %>%
  summarise(mean.Population = mean(Population),
            total.Population = sum(Population))

xtable1 <- xtable(summary_table, label="tab:tab1", caption="Selections")
print.xtable(xtable1, comment=FALSE, include.rownames=FALSE)
```

We can also make exploratory plots of several continuous data points to determine possible relationships, as well as covariance among explanatory variables. 

```{r}
#Drop geo info
aux5 <- st_set_geometry(PM2.5_Full_Elev_utm, value=NULL)

PM2.5_Variables <- 
  aux5 %>%
  select(PM2.5, Population, PM10:elevation) %>%
  na.omit()

PM2.5_VariablesCorr <- cor(PM2.5_Variables)
corrplot(PM2.5_VariablesCorr, method = "ellipse") # without ellipse it gives you circles
corrplot.mixed(PM2.5_VariablesCorr, upper = "ellipse")
```

We determine the degree of temporal autocorrelation in our dataset.

```{r}
PM252018.auto <- lme(data = PM2.5_Full_Elev_Week_utm, PM2.5 ~ Date + Population + as.factor(Zone) + PM10 + TAVG + Emiss_Dist + elevation, random = ~1|Week)

#Determine the temporal autocorrelation in the model
ACF(PM252018.auto)
```

From the ACF output, the 2nd value is taken (0.232) to define the degree of autocorrelation. Then, a repeated measures ANOVA model is created with the defined autocorrelation structure. the model is considering temporal autocorrelation within the levels of Week, and we have retained Week as a random effect.  

```{r}
PM252018.mixed <- lme(data = PM2.5_Full_Elev_Week_utm, PM2.5 ~ Date + Population + as.factor(Zone) + PM10 + TAVG + Emiss_Dist + elevation, random = ~1|Week, correlation = corAR1(form = ~ Date|Week, value = 0.2321332919), method = "REML")
  
class(PM2.5_Full_Elev_Week_utm$Date)  
  
  
  
  lme(data = PeterPaul.summertemp,
                     temperature_C ~ sampledate * lakename, 
                     random = ~1|Week,
                     #specify autocorrelation structure of order 1 (this is the 42.3%)
                     #sampledate is duplicated in some cases, so need to split up by lake. 
                     #FRA: This is only beacuse R doesnt like  that the sample were taken the same time.
                     correlation = corAR1(form = ~ sampledate/lakename|Week, value = 0.423), 
                     # we put manually the 0.423
                     #define method as restricted maximum likelihood
                     method = "REML")
summary(TempTest.mixed)

#Results: Random we got a stddev. Fixed effects: lakenamePeter Lake coef --> no diff between lakes. 
# The interaction is also not significant.

# Compare the random effects model with the fixed effects model
TempTest.fixed <- gls(data = PeterPaul.summertemp,
                      temperature_C ~ sampledate * lakename, 
                      method = "REML")
summary(TempTest.fixed)

# little diff results but same conclusions
# why did we wanto to use random effect? --> We now there is variability. why dont use 
# that for our advantage.

anova(TempTest.mixed, TempTest.fixed)
# The lower the AIC, the better.
# The p-value tells us whether those models have a significantly different fit

# FRA: the models and signif diff and the mixed is better (AIC)

# Post-hoc test
# This will yield groupings of temperature by lake for the average date value
TempTest.posthoc = lsmeans(TempTest.mixed, ~ sampledate * lakename)
cld(TempTest.posthoc, alpha = 0.05, Letters = letters, adjust = "tukey")
# FRA: you want this is there is a signif diff between groups. Something about groups.


# display our final relationship
ggplot(PeterPaul.summertemp, aes(x = sampledate, y = temperature_C, color = lakename)) +
  geom_point() +
  scale_color_manual(values = c("#7fcdbb", "#253494")) +
  geom_abline(intercept = 20.47, slope = 0.0001) #rounded coeff.
```

```{r}
#Adding the Week variable
PM2.5_Full_Elev_Week_utm <- 
  PM2.5_Full_Elev_utm %>%
  mutate(Week = week(Date)) %>%
  na.omit()

PM2.5_Full_Elev_Week_utm$TAVG <- as.numeric(PM2.5_Full_Elev_Week_utm$TAVG)
```